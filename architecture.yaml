# Concept MRI - Architecture Tracking Document
# Status: todo | in_progress | done | blocked
# Priority: critical | high | medium | low
#
# ✅ MAJOR MILESTONE: Complete MoE Data Capture System Working!
# - All 9/9 integration tests passing with real GPT-OSS-20B model
# - Session-based capture with coordinated schema writing
# - Highway analysis generating signatures like "L0E6→L1E28→L2E14"  
# - Data lake with 30 records per schema (routing, expert internal, expert output)
# - Adapted successfully to quantized Mxfp4GptOssExperts architecture
# ✅ NEW MILESTONE: Complete End-to-End Probe Creation and Data Capture!
# - React frontend with probe creation UI fully working
# - Session creation → execution → finalization pipeline complete
# - Real activation data captured: 360 records with 2880-dimensional vectors
# - All Parquet files written with substantial data (2MB+ activation files)
# - Session state management and UI integration working properly

meta:
  project: "Concept MRI - OpenAI Hackathon"
  version: "1.6"
  target_model: "gpt-oss-20b (OpenAI MoE)"
  model_details:
    - "1.8B total parameters (inspected: not 21B as initially assumed)"
    - "24 layers, 32 experts per layer, K=4 routing (not K=1)" 
    - "Uses MXFP4 quantization, fits in 16GB GPU memory"
    - "Available at: openai/gpt-oss-20b on Hugging Face"
    - "Apache 2.0 license"
    - "Analysis Strategy: Capture K=4 routing, analyze top-1 expert per layer"
    - "CRITICAL: Quantized model uses Mxfp4GptOssExperts - no individual expert modules"
    - "Hook Strategy: MLP combined hook for routing + experts collective hook for processing"
    - "Individual expert analysis not possible - collective expert patterns only"
    - "NEW: Multi-category word membership for rich context disambiguation demos"
    - "NEW: Complete FastAPI server with 5 working probes endpoints"
  timeline: "7 days"
  updated: "2025-01-06"
  constraints:
    - "Two token inputs only"
    - "Several thousand words max"
    - "User-supplied API keys for LLM labeling"
    - "K=4 routing: Capture all 4 experts, analyze top-1 weighted expert per layer"
    - "Highway paths: Top-1 expert chains (e.g. L6E2→L7E15→L8E7)"
    - "Future expansion: Full K=4 expert combination analysis available"

# Critical Workflow Clarifications
workflow:
  probe_vs_experiment:
    - "**Probes populate data lake:** Run MoE capture on contexts → write Parquet (routing + PCA128)"
    - "**Experiments analyze word lists:** Select word subsets from populated data for analysis"
    - "**NO probe re-running:** Experiments consume existing captures, never re-run probes"
    - "**LLM labeling:** Applied to BOTH cluster cards AND archetypal path narratives"

  data_flow:
    - "Probe: contexts → MoE routing → lake artifacts (reusable)"
    - "Experiment: word lists → highway selection → cohort export → clustering/CTA → labeling"
    - "Analysis: clusters get LLM labels, paths get LLM narratives"
    - "Dual Transformation Analysis:"
    - "  1) Expert Highway Routing: How contexts transform MoE routing patterns across experts"
    - "  2) Individual Highway Latent: Context→target transformation within specific highway's space"
    - "Multi-Experiment Demo: Run separate experiments to showcase each transformation type"

# Core Infrastructure & Setup
infrastructure:
  directory_structure:
    status: done
    priority: critical
    description: "Root project directory organization"
    layout: |
      OpenAIHackathon-ConceptMRI/
      ├── backend/               # Python backend
      │   ├── src/              # Source code
      │   ├── tests/            # Backend tests
      │   └── requirements.txt  # Python deps
      ├── frontend/             # React frontend
      ├── data/                 # Data storage
      │   ├── lake/            # Probe captures
      │   ├── experiments/     # Experiment artifacts
      │   └── models/          # MoE model cache
      ├── notebooks/           # Dev notebooks
      ├── samples/             # Test data
      ├── scripts/             # Setup/maintenance
      └── docker-compose.yml   # Container orchestration

  model_setup:
    status: done
    priority: critical
    description: "gpt-oss-20b model download and testing"
    tasks:
      - name: "System requirements check"
        status: done
        file: "scripts/check_system.py"
        notes: "RTX 5070 Ti 16GB confirmed adequate"
      - name: "Model download script"
        status: done
        file: "scripts/setup_gpt_oss_20b.py"
      - name: "Model download (20GB)"
        status: done
        notes: "Downloaded from openai/gpt-oss-20b"
      - name: "Basic model loading test"
        status: done
        file: "scripts/test_quantized_model.py"
        notes: "Working with MXFP4 quantization on RTX 5070 Ti (~5s load time, ~14GB GPU)"
      - name: "MoE architecture inspection"
        status: done
        file: "scripts/inspect_moe_structure.py"
        notes: "24 layers, 32 experts/layer, K=4 routing, GptOssTopKRouter class identified"
      - name: "Activation hook development"
        status: done
        files: ["scripts/test_routing_capture_fixed.py", "scripts/test_routing_capture.py", "scripts/test_routing_minimal.py"]
        notes: "EnhancedRoutingCapture class implemented for quantized MoE. MLP combined hook captures routing directly, experts collective hook captures post-expert processing."

  project_setup:
    status: done
    priority: critical
    tasks:
      - name: "Initialize repository structure"
        status: done
        files: ["README.md", "requirements.txt", "pyproject.toml"]
      - name: "Set up development environment"
        status: done
        files: [".env.example", "docker-compose.yml", "Makefile"]
      - name: "Configure logging and error handling"
        status: todo
        priority: critical
        files: ["backend/src/utils/logging.py", "backend/src/utils/errors.py"]
        hackathon_scope:
          logging:
            - "Single JSON logger with timestamps"
            - "Console output only (no file rotation for MVP)"
            - "Three levels: ERROR, INFO, DEBUG"
            - "Request timing and memory usage"
            - "MoE routing decisions and clustering progress"
          error_handling:
            - "Try/catch with context logging"
            - "Graceful degradation (skip failed clusters, continue)"
            - "User-friendly error messages in API responses"

  data_layer:
    status: done
    priority: critical
    description: "Parquet storage with integrated capture service and token position tracking"
    tasks:
      - name: "Data schemas and contracts"
        status: done
        files: ["backend/src/schemas/"]
        completed_files:
          - "backend/src/schemas/tokens.py" # Updated with capture_session_id
          - "backend/src/schemas/routing.py" # Updated with token_position tracking
          - "backend/src/schemas/expert_internal_activations.py" # Updated for quantized MoE collective experts
          - "backend/src/schemas/expert_output_states.py" # Updated with token_position
          - "backend/src/schemas/features_pca128.py" # For experiment-time PCA
          - "backend/src/schemas/capture_manifest.py" # Session metadata for UI integration
      - name: "Parquet I/O utilities"
        status: done
        files:
          - "backend/src/core/parquet_writer.py"
          - "backend/src/core/parquet_reader.py"
          - "backend/src/core/data_lake.py"
      - name: "Shared utilities"
        status: done
        files:
          - "backend/src/utils/numpy_utils.py"
          - "backend/src/utils/parquet_utils.py"
          - "backend/src/utils/errors.py"
          - "backend/src/utils/memory_utils.py"
      - name: "WordNet mining utility"
        status: done
        file: "backend/src/utils/wordnet_mining.py"
        description: "WordNet-based semantic category mining with unambiguous word filtering and POS-pure extraction"
        completed_features:
          - "Mine hyponyms from synsets with max_depth parameter (e.g., animal.n.01 → cat, dog, mouse)"
          - "Filter for globally unambiguous words (single word sense only) for clean demos"
          - "Single-token validation with tokenizer integration"
          - "POS-pure word mining (words that are ONLY nouns, ONLY verbs, etc.)"
          - "All-words mining (including ambiguous words for broader analysis)"
          - "Multi-category mining with automatic labeling"
          - "Full synset ID transparency for research traceability"
        test_results:
          - "30 pure nouns vs 30 pure verbs found for POS contrast analysis"
          - "22 unambiguous animals vs 513 all animals (including ambiguous)"
          - "Integration tested with real GPT-OSS-20B captures"
      - name: "Manifest and ID system"
        status: todo
        files: ["backend/src/core/manifests.py", "backend/src/core/ids.py", "backend/src/core/types.py"]

# Backend Services (Python FastAPI)
backend:
  api_server:
    status: done
    priority: critical
    files: ["backend/src/api/main.py", "backend/src/api/routers/probes.py", "backend/src/api/schemas.py", "backend/src/api/dependencies.py"]
    completed_features:
      - "FastAPI server with CORS middleware (allow_origins=['*'] for development)"
      - "Startup model loading with FastAPI lifespan events - reliable single initialization"
      - "Model dependency injection with absolute path resolution matching test scripts"
      - "Complete probes router with 5 endpoints"
      - "Multi-category WordSource support integrated with IntegratedCaptureService"
      - "Fixed model loading instability - startup loading eliminates concurrent initialization issues"
      - "Session lifecycle management (create → execute → status)"
    endpoints:
      # PROBES (session-based capture with multi-category support)
      - path: "/api/probes"
        method: "POST"
        status: done
        priority: critical
        body: "{ session_name:str, context_sources:[WordSource], target_sources:[WordSource] }"
        returns: "{ session_id, total_pairs, contexts:[str], targets:[str], categories:{} }"
        description: "Create session from WordSource objects (custom, pos_pure, synset_hyponyms). Uses create_session_from_sources() with multi-category preservation."

      - path: "/api/probes/{session_id}/execute"
        method: "POST"
        status: done
        priority: critical
        returns: "{ started:bool, probe_ids:[str], status_url:str }"
        description: "Execute all context-target pairs for session using capture_session_batch()."

      - path: "/api/probes/{session_id}/status"
        method: "GET"
        status: done
        priority: critical
        returns: "{ session_id, state, progress:{}, manifest?:{}, data_lake_paths?:{} }"
        description: "Monitor session progress. Auto-finalizes and returns manifest + paths when completed."

      - path: "/api/probes"
        method: "GET"
        status: done
        priority: high
        returns: "[{ session_id, session_name, created_at, probe_count, contexts:[str], targets:[str], state }]"
        description: "List all available capture sessions for experiment creation."

      - path: "/api/probes/{session_id}"
        method: "GET"
        status: done
        priority: high
        returns: "{ manifest:{}, data_lake_paths:{}, categories:{} }"
        description: "Get complete session details including full manifest and Parquet file paths."
        bug_fixes:
          - "Fixed category field name mismatch - API now reads context_category_assignments and target_category_assignments from stored metadata"
          - "Fixed 404 errors for completed sessions by handling both active and finalized sessions"
          - "Enhanced error handling with separate ValueError, FileNotFoundError, and generic exception paths"
    test_validation:
      - "Server starts successfully with GPT-OSS-20B model loading"
      - "Session creation works with real WordSource mining"
      - "Multi-category assignments preserved in API responses"
      - "All endpoints return proper HTTP status codes and error handling"

      # EXPERIMENTS (consume captures -> highways/cohort/cluster/cta)
      - path: "/api/experiments"
        method: "POST"
        status: todo
        priority: critical
        body: "{ capture_id:str, label?:str }"
        returns: "{ experiment_id }"
        description: "Create an experiment bound to an existing capture. No capture re-run."

      - path: "/api/experiments/{id}/highways"
        method: "POST"
        status: todo
        priority: critical
        returns: "{ highways_json_path, stats, window_used:[int,int,int] }"
        description: "Compute ExpertHighways from the experiment's capture (first window auto-selected)."

      - path: "/api/experiments/{id}/cohort"
        method: "POST"
        status: todo
        priority: critical
        body: "{ highway_signature:'L6E2→L7E3→L8E1', contexts:[str], mode?:'single'|'multi' }"
        returns: "{ cohort_path }"
        description: "Export cohort for selected highway+context(s). Enhanced to support context-target filtering."

      - path: "/api/experiments/{id}/highway-analysis"
        method: "GET"
        status: todo
        priority: critical
        params: "{ highway_signature:str }"
        returns: "{ contexts_targets:{ [context]:{ targets:[str], share:float } } }"
        description: "Get (context, target) pairs that flow through specified highway. Simple data query."


      - path: "/api/experiments/{id}/cluster"
        method: "POST"
        status: todo
        priority: critical
        body: "{ algo:'kmeans'|'hierarchical', k_per_layer:{L6:int,...} }"
        returns: "{ models:[...], assignments_paths:[...], window_used:[int,int,int] }"
        description: "Cluster cohort features on the auto-selected first window."

      - path: "/api/experiments/{id}/cta"
        method: "POST"
        status: todo
        priority: critical
        returns: "{ paths_path, survival_path, window_used:[int,int,int] }"
        description: "CTA (macro paths + survival/confusion) on first window."

      - path: "/api/experiments/{id}/rules/ets"
        method: "POST"
        status: todo
        priority: high
        returns: "{ rules_path }"
        description: "Generate ETS rules (faithful thresholds + prec/cov + CIs)."
        
      - path: "/api/experiments/{id}/rules/clr"
        method: "POST"
        status: todo
        priority: high
        returns: "{ rules_path }"
        description: "Generate CLR rules (lineage rules + prec/cov)."

      - path: "/api/experiments/{id}/label"
        method: "POST"
        status: todo
        priority: high
        returns: "{ clusters_path }"
        description: "Generate LLM labels for clusters (short_label, dominant, secondary, outliers, provenance)."

      - path: "/api/experiments"
        method: "GET"
        status: todo
        priority: high
        returns: "[{ id, created_at, capture_id, label?, window_used?, last_stage? }]"
        description: "List experiments for quick open in UI."

      - path: "/api/experiments/{id}"
        method: "GET"
        status: todo
        priority: high
        returns: "{ manifest, artifact_paths }"
        description: "Manifest/artifact lookup for loaders."
        
      # EXPERT HIGHWAY ROUTING TRANSFORMATION
      - path: "/api/transform/routing-matrix"
        method: "POST"
        status: todo
        priority: high
        body: "{ capture_id:str, contexts:[str] }"
        returns: "{ labels:[str], matrix:[[float]], mode:'routing' }"
        description: "Expert Highway Routing Analysis: How different contexts transform MoE routing patterns."

      # INDIVIDUAL HIGHWAY LATENT TRANSFORMATION  
      - path: "/api/experiments/{id}/highway-latent-transform"
        method: "POST"
        status: todo
        priority: high
        body: "{ highway_signature:str, context_token:str, target_tokens:[str] }"
        returns: "{ transformation_data:[[float]], target_labels:[str], basis:'latent_space' }"
        description: "Individual Highway Latent Analysis: Context→target transformation within specific highway's space."

      # EXPERT INTERNAL CLUSTERING
      - path: "/api/experiments/{id}/expert-internal-clustering"
        method: "POST"
        status: todo
        priority: high
        body: "{ layer:int, clustering_algo:'kmeans'|'hierarchical', k:int }"
        returns: "{ clusters, pca_3d_data, collective_analysis, cluster_cards }"
        description: "Analyze collective expert processing patterns with 3D PCA and clustering (quantized MoE limitation)."
        notes: "Individual expert analysis not possible with Mxfp4GptOssExperts - collective patterns only"

      # EXPERT OUTPUT CLUSTERING + CTA INTEGRATION
      - path: "/api/experiments/{id}/expert-output-clustering"
        method: "POST"
        status: todo
        priority: high
        body: "{ clustering_algo:'kmeans'|'hierarchical', k:int, auto_cta:boolean }"
        returns: "{ clusters, pca_3d_data, cta_results?, trajectory_sankey_data }"
        description: "Cluster collective expert output latent space with optional real-time CTA integration."

      # LLM INSIGHTS AND REPORT GENERATION
      - path: "/api/experiments/{id}/llm-insights"
        method: "POST"
        status: todo
        priority: high
        body: "{ analysis_type:'summary'|'expert_specialization'|'trajectory_analysis'|'custom', custom_prompt?:str }"
        returns: "{ insights, generated_analysis, report_sections, visualizations }"
        description: "Generate AI-powered insights and analysis reports for expert and trajectory findings."

      - path: "/api/experiments/{id}/llm-insights/ask"
        method: "POST"
        status: todo
        priority: high
        body: "{ question:str, context_data?:object }"
        returns: "{ answer, supporting_evidence, related_findings }"
        description: "Custom question-answering about experiment analysis results."

  services:
    # PROBES
    integrated_capture_service:
      status: done
      priority: critical
      file: "backend/src/services/probes/integrated_capture_service.py"
      description: "Complete session-based MoE capture with WordNet integration and multi-category probe support."
      supporting_files: 
        - "backend/src/services/probes/routing_capture.py" # EnhancedRoutingCapture with quantized MoE hooks
        - "backend/src/services/probes/probe_ids.py" # Probe and session ID generation
        - "backend/src/utils/wordnet_mining.py" # WordNet integration for category mining
        - "scripts/test_integrated_capture.py" # Original comprehensive test
        - "scripts/test_integrated_wordnet_mining.py" # WordNet integration test
        - "scripts/test_multi_category_probe.py" # Multi-category demo scenarios
        - "scripts/test_end_to_end_category_parquet.py" # End-to-end Parquet validation
      features:
        - name: "Quantized MoE architecture adaptation"
          status: done
          notes: "Successfully adapted to Mxfp4GptOssExperts structure - no individual expert modules, collective processing only"
        - name: "WordNet mining integration"
          status: done
          notes: "Integrated WordNetMiner for flexible word source mining: custom lists, POS-pure words, synset hyponyms"
        - name: "Multi-category session creation"
          status: done
          notes: "create_session_from_sources() method supports multiple word sources with automatic category assignment and preserves all category memberships"
        - name: "Category assignment storage"
          status: done
          notes: "Multi-category assignments (Dict[str, List[str]]) stored in CaptureManifest and survive Parquet serialization/deserialization with full category preservation"
        - name: "Session-based capture management" 
          status: done
          notes: "Complete session lifecycle: create -> capture -> finalize. Supports both simple and multi-category sessions."
        - name: "Word source flexibility (_mine_from_source helper)"
          status: done
          notes: "Supports 3 word sources: custom (user lists), pos_pure (grammatically unambiguous), synset_hyponyms (semantic categories)"
        - name: "MLP combined hook (routing + output capture)"
          status: done
          notes: "Single hook captures both routing computation and MLP output. Replicates router logic manually since router.forward() never called."
        - name: "Experts collective hook (post-expert processing)"
          status: done
          notes: "Captures collective expert processing output for latent analysis. Works with flattened tensor shapes."
        - name: "Expert highway extraction with top-1 from K=4 routing"
          status: done
          notes: "highway_signature() method creates L{layer}E{expert} signatures focusing on target tokens (position=1)"
        - name: "Token position tracking for highway analysis"
          status: done
          notes: "All schemas updated with token_position field (0=context, 1=target) for proper highway analysis"
        - name: "Coordinated schema writing with BatchWriters"
          status: done
          notes: "SessionBatchWriters coordinates all 5 schemas with proper probe_id foreign key relationships"
        - name: "Data lake integration and validation"
          status: done
          notes: "Complete data flow: hooks -> schemas -> parquet files -> highway analysis. All tests passing including category assignments."
        - name: "GPU memory management and error resilience"
          status: done
          notes: "Proper hook cleanup, CUDA memory clearing, graceful failure handling"
      test_validation:
        - "9/9 integration tests passing with real GPT-OSS-20B model"
        - "Multi-category sessions tested with 2/3 demo scenarios ready (≥20 pairs each)"
        - "End-to-end Parquet integration verified with category assignments"
        - "Context disambiguation demo configuration specified (72 probes, 4-5 minutes)"
      dependencies: ["transformers", "bitsandbytes", "accelerate", "nltk"]

    # EXPERIMENTS
    highways_service:
      status: todo
      priority: critical
      file: "backend/src/services/experiments/highways.py"
      description: "Compute P(Eℓ+1|Eℓ) using top-1 experts from K=4 routing data."
      features:
        - name: "First window auto-select & persist to experiment manifest"
          status: todo
        - name: "Top-1 expert transition probabilities P(E_top1_l+1|E_top1_l)"
          status: todo
          notes: "Use expert_top1_id field from routing schema for highway computation"
        - name: "Coverage and stickiness metrics (top-1 expert paths)"
          status: todo
        - name: "Highway signature format: L{layer}E{top1_expert} (e.g. L6E2→L7E15→L8E7)"
          status: todo
        - name: "ExpertHighways.json export with top-1 routing"
          status: todo
        - name: "Optional: K=4 full routing analysis for future expansion"
          status: todo
          notes: "Store full expert_top4_ids data for potential multi-expert highway analysis"

    cohort_service:
      status: todo
      priority: critical
      file: "backend/src/services/experiments/cohorts.py"
      description: "Select probe_ids by highway_signature+context(s); write cohort parquet in experiment. Supports both single-context and within-highway multi-context analysis."
      highway_signature_format: "L{layer}E{expert} notation (e.g., L6E2→L7E3→L8E1)"
      features:
        - name: "Highway signature matching"
          status: todo
        - name: "Single context cohort export (original workflow)"
          status: todo
        - name: "Multi-context cohort export (within-highway analysis)"
          status: todo
          notes: "Export cohort containing multiple contexts but constrained to same highway"
        - name: "Highway analysis: (context, target) pair enumeration"
          status: todo
          notes: "For Highway Panel display - which contexts and targets use this highway"
        - name: "Cohort manifest badges (highway_signature, context_tokens, mode)"
          status: todo

    cluster_service:
      status: todo
      priority: critical
      file: "backend/src/services/experiments/cluster/"
      description: "Cluster cohort features (first window only)."
      components:
        - name: "base.py (ClusterBackend interface)"
          status: todo
        - name: "kmeans.py (MiniBatch)"
          status: todo
        - name: "hierarchical.py (Ward)"
          status: todo
      features:
        - name: "k-per-layer"
          status: todo
        - name: "Model + assignments persistence"
          status: todo

    collective_expert_clustering_service:
      status: todo
      priority: high
      file: "backend/src/services/experiments/collective_expert_clustering.py"
      description: "Cluster collective expert processing outputs from quantized MoE (individual expert analysis not possible)."
      features:
        - name: "Extract collective expert outputs for specific layers"
          status: todo
          notes: "Get collective expert processing states from Mxfp4GptOssExperts"
        - name: "3D PCA analysis of collective expert concept organization"
          status: todo
        - name: "K-means/hierarchical clustering of collective expert patterns"
          status: todo
        - name: "Expert specialization analysis (what concepts this expert handles)"
          status: todo
        - name: "Generate expert internal cluster cards with LLM labeling"
          status: todo

    expert_output_clustering_service:
      status: todo
      priority: high
      file: "backend/src/services/experiments/expert_output_clustering.py"
      description: "Cluster collective expert output latent space and integrate with CTA analysis."
      features:
        - name: "Collect expert output states across layers (collective latent space)"
          status: todo
        - name: "3D PCA visualization of expert output latent space"
          status: todo
        - name: "Interactive clustering with real-time CTA integration"
          status: todo
          notes: "Clustering results automatically feed into CTA analysis"
        - name: "Concept trajectory analysis through clustered latent regions"
          status: todo
        - name: "Sankey diagram generation for cluster transitions"
          status: todo

    expert_clustering_service:
      status: todo
      priority: medium
      file: "backend/src/services/experiments/expert_clustering.py"
      description: "Cluster experts by usage patterns and analyze token paths through expert clusters."
      features:
        - name: "Expert usage pattern analysis"
          status: todo
          notes: "Analyze which experts are used together across tokens"
        - name: "Expert similarity clustering" 
          status: todo
          notes: "Cluster experts by co-activation patterns and routing behavior"
        - name: "Token path analysis through expert clusters"
          status: todo
          notes: "Track tokens that use same expert clusters vs those that diverge"
        - name: "Expert cluster transition visualization"
          status: todo
          notes: "Show how tokens move between expert clusters across layers"
        - name: "Expert specialization convergence analysis"
          status: todo
          notes: "Identify tokens that converge to same expert clusters (similar processing)"

    cta_service:
      status: todo
      priority: critical
      file: "backend/src/services/experiments/cta.py"
      description: "CTA macro paths + survival/confusion (≥5% coverage)."
      features:
        - name: "Paths + examples writer"
          status: todo
        - name: "Survival/confusion metrics"
          status: todo
        - name: "5% coverage filter"
          status: todo

    rules_service:
      status: todo
      priority: high
      files: ["src/services/experiments/rules/ets.py", "src/services/experiments/rules/clr.py"]
      description: "ETS (faithful) + CLR (lineage) for cluster cards."
      features:
        - name: "Explainable Threshold Similarity"
          status: todo
        - name: "Cluster Lineage Rules"
          status: todo
        - name: "Precision/coverage with CIs"
          status: todo

    labeling_service:
      status: todo
      priority: high
      file: "backend/src/services/experiments/labeling.py"
      description: "LLM + stats → cards AND archetypal path narratives. Two-stage process: 1) Label clusters, 2) Use cluster labels to generate path narratives."
      features:
        - name: "Cluster semantic labeling with ontology grouping"
          status: todo
          details: "Dominant/secondary/outlier analysis with structured JSON output"
        - name: "Archetypal path narrative generation"
          status: todo
          details: "Takes cluster labels + path info → generates conceptual journey narratives"
        - name: "Two-stage LLM workflow"
          status: todo
          details: "Stage 1: Cluster labeling → Stage 2: Path labeling (uses cluster results)"
        - name: "Provenance tracking (which LLM generated which labels)"
          status: todo
        - name: "Structured output validation"
          status: todo
          details: "Validate JSON schemas for both cluster and path labels"

    llm_insights_service:
      status: todo
      priority: high
      file: "backend/src/services/experiments/llm_insights.py"
      description: "AI-powered analysis and report generation for expert specialization and concept trajectory findings."
      features:
        - name: "Automated expert specialization analysis"
          status: todo
          notes: "Generate insights about what each expert has learned to handle"
        - name: "Context transformation pattern analysis"
          status: todo
          notes: "Analyze how different contexts change expert routing patterns"
        - name: "Concept trajectory narrative generation"
          status: todo
          notes: "Create coherent stories about concept evolution through latent space"
        - name: "Custom question answering about analysis results"
          status: todo
          notes: "Allow scientists to ask specific questions about their findings"
        - name: "Research report compilation and export"
          status: todo
          notes: "Generate comprehensive reports combining all analysis types"
        - name: "Multi-domain correlation insights"
          status: todo
          notes: "Find relationships between expert specialization, highway patterns, and latent trajectories"

    expert_routing_transform_service:
      status: todo
      priority: high
      file: "backend/src/services/transform/expert_routing.py"
      description: "Expert Highway Routing Transformation: How contexts change MoE routing patterns."
      features:
        - name: "routing_distribution(capture_id, context)"
          status: todo
        - name: "context_routing_similarity_matrix(contexts)"
          status: todo
        - name: "expert_transition_differences(context_a, context_b)"
          status: todo

    highway_latent_transform_service:
      status: todo
      priority: high
      file: "backend/src/services/transform/highway_latent.py"
      description: "Individual Highway Latent Transformation: Context→target analysis within highway's space."
      features:
        - name: "extract_highway_context_targets(experiment_id, highway_signature, context)"
          status: todo
          notes: "Find targets that pair with specific context in this highway"
        - name: "compute_context_target_trajectories(context, targets, highway_signature)"
          status: todo
          notes: "Analyze how context transforms target trajectories in highway's latent space"
        - name: "latent_transformation_clustering(context_target_cohort)"
          status: todo
          notes: "Cluster on context to show target trajectory shifts"

    bundle_service:
      status: todo
      priority: low
      file: "backend/src/services/bundle.py"
      description: "Experiment bundling and replay"
      features:
        - name: "Zip experiment directory"
          status: todo
        - name: "Manifest validation"
          status: todo
        - name: "Replay without hidden state"
          status: todo

# Frontend (React + TypeScript)
frontend:
  setup:
    status: done
    priority: critical
    description: "React + Vite + TypeScript + Tailwind CSS with API integration"
    files: ["frontend/package.json", "frontend/vite.config.ts", "frontend/tailwind.config.js", "frontend/postcss.config.js"]
    completed_features:
      - "React 19.1.1 + TypeScript with Vite 7.1.4 build system"
      - "Tailwind CSS 4.1.13 with PostCSS integration (@tailwindcss/postcss)"
      - "TypeScript interfaces matching backend Pydantic schemas"
      - "API client with error handling and utility methods"
      - "Working API connection test interface"
      - "Fixed verbatimModuleSyntax: true compatibility with import type syntax"
      - "Visualization libraries: ECharts 5.6.0, Plotly.js 3.1.0, React Router 7.8.2"
    technical_notes:
      - "TypeScript config uses verbatimModuleSyntax: true requiring import type {} for interfaces"
      - "API types exported with export type {} syntax for Vite compatibility"
      - "PostCSS configuration properly set up for Tailwind 4.x"
      - "Development server runs on localhost:5174 with hot module replacement"
      - "All TypeScript interface imports resolved with proper type-only import syntax"
    files_implemented:
      - "frontend/src/types/api.ts - TypeScript interfaces for all backend schemas"
      - "frontend/src/api/client.ts - Complete API client with ConceptMriApiClient class"
      - "frontend/src/App.tsx - Complete React Router setup with WorkspacePage and ExperimentPage routes"
      - "frontend/src/pages/WorkspacePage.tsx - Complete workspace interface with ActionCard and Modal components"
      - "frontend/src/components/ActionCard.tsx - Reusable action card component with 4 color variants"
      - "frontend/src/components/Modal.tsx - Accessible modal component replacing browser alerts"
      - "frontend/src/components/icons/Icons.tsx - SVG icon components (Flask, ChartBar, Clock, ChartPie)"
      - "frontend/src/constants/workspace.ts - Constants for magic numbers (MAX_RECENT_SESSIONS)"
      - "frontend/src/pages/ExperimentPage.tsx - Complete experiment analysis interface with word filtering"
      - "frontend/src/components/WordFilterPanel.tsx - Category-based word filtering with checkboxes"
      - "frontend/src/components/FilteredWordDisplay.tsx - Display filtered words with their categories"
      - "frontend/tailwind.config.js - Vite-compatible configuration"
      - "frontend/postcss.config.js - Fixed @tailwindcss/postcss plugin setup"

  pages:
    workspace_page:
      status: done
      priority: critical
      file: "frontend/src/pages/WorkspacePage.tsx"
      description: "Main interface for probe/experiment management and setup - refactored with reusable components"
      features:
        - name: "New Probe Button"
          status: done
          notes: "Opens modal dialog (placeholder for future WordNet integration)"
        - name: "New Experiment Button"
          status: done
          notes: "Navigates to /experiment route with ExperimentPage component"
        - name: "Recent Sessions List"
          status: done
          notes: "Displays up to 5 recent sessions with loading states and error handling"
        - name: "Statistics Card"
          status: done
          notes: "Shows session count, probe count, and last activity"
        - name: "Status Bar"
          status: done
          notes: "Backend connection status with refresh button"
      components_used:
        - "ActionCard component for New Probe and New Experiment"
        - "Modal component for dialogs (replaces browser alerts)"
        - "Reusable SVG icons (FlaskIcon, ChartBarIcon, ClockIcon, ChartPieIcon)"
        - "Constants from workspace.ts (MAX_RECENT_SESSIONS)"

    experiment_page:
      status: done
      priority: critical
      file: "frontend/src/pages/ExperimentPage.tsx"
      description: "Complete analysis interface with session selector, word filtering, and dual-tab layout with interactive Sankey-driven cards"
      layout: |
        ExperimentPage Layout:
        ├── Left Sidebar (w-80):
        │   ├── Session Selector (dropdown for completed sessions)
        │   ├── Analysis Type Tabs (Expert Highways | Latent Space)
        │   ├── WordFilterPanel (category checkboxes with counts)
        │   ├── FilteredWordDisplay (context/target words with categories)
        │   └── Session Info (probe count, status)
        ├── Main Content Area:
        │   ├── Expert Highways Tab:
        │   │   ├── Layout: Controls (left) | Interactive Expert Sankey (center) | Context-sensitive Cards (right)
        │   │   │   ├── Left: Color controls + visualization config
        │   │   │   ├── Center: Expert Routing Sankey (click experts/routes)
        │   │   │   └── Right: Expert Card (click expert) OR Highway Card (click route)
        │   ├── Latent Space Tab:
        │   │   ├── Layout: Controls (left) | Interactive Trajectory Sankey + PCA 3D (center) | Context-sensitive Cards (right)
        │   │   │   ├── Left: Clustering controls + Color controls
        │   │   │   ├── Center: Trajectory Sankey + PCA 3D plot (click clusters/routes)
        │   │   │   └── Right: Cluster Card (click cluster) OR Route Card (click trajectory)
        │   └── Bottom: LLM Analysis Panel (shared between tabs)
        └── Interaction Pattern: Select session → Filter words → Configure visualization → Click Sankey elements → See context-sensitive cards → Generate LLM analysis

  components:
    action_card:
      status: done
      priority: critical
      file: "frontend/src/components/ActionCard.tsx"
      description: "Reusable card component for main actions with icons, descriptions, and buttons"
      features:
        - "4 color variants (blue, green, purple, orange)"
        - "Loading and disabled states"
        - "TypeScript interfaces with proper prop typing"
        - "Consistent Tailwind styling with hover effects"
        - "Accessibility with ARIA labels and focus states"

    modal:
      status: done
      priority: critical  
      file: "frontend/src/components/Modal.tsx"
      description: "Reusable modal component replacing browser alerts"
      features:
        - "Backdrop click to close"
        - "Escape key support"
        - "ARIA modal attributes (role, aria-modal, aria-labelledby)"
        - "Portal-like overlay with z-index management"
        - "TypeScript props with ReactNode children"

    icons:
      status: done
      priority: critical
      file: "frontend/src/components/icons/Icons.tsx" 
      description: "Reusable SVG icon components with accessibility"
      features:
        - "FlaskIcon, ChartBarIcon, ClockIcon, ChartPieIcon"
        - "Consistent sizing with customizable className prop"
        - "ARIA labels and role attributes"
        - "Extracted from inline SVG bloat in WorkspacePage"

    constants:
      status: done
      priority: low
      file: "frontend/src/constants/workspace.ts"
      description: "Constants for WorkspacePage to replace magic numbers"
      features:
        - "MAX_RECENT_SESSIONS = 5 (replaces hardcoded .slice(0, 5))"
        - "Simple approach - only essential constants for MVP"

    word_filter_panel:
      status: done
      priority: critical
      file: "frontend/src/components/WordFilterPanel.tsx"
      description: "Category-based word filtering with interactive checkboxes and counts - React hooks properly configured"
      features:
        - "FilterState interface with Set<string> for efficient lookups"
        - "Dynamic category counting from SessionDetailResponse data"
        - "Context and target category sections with individual toggles"
        - "Select All / Clear All functionality for each category type"
        - "Live filtering pair count display and current filter summary"
        - "Proper TypeScript integration with useMemo for performance"
        - "Fixed React hoisting issue - filtering logic moved inline to useMemo"

    filtered_word_display:
      status: done
      priority: critical
      file: "frontend/src/components/FilteredWordDisplay.tsx"
      description: "Display filtered context and target words with their categories"
      features:
        - "Real-time filtering based on FilterState from WordFilterPanel"
        - "Separate context and target word sections with counts"
        - "Word-category mapping display (e.g., 'cat' [animals, pos])"
        - "Total possible pairs calculation for filtered results"
        - "Loading states and empty data handling"
        - "Integration with SessionDetailResponse category structure"

    new_probe_dialog:
      status: done
      priority: critical
      file: "frontend/src/components/NewProbeDialog.tsx" 
      description: "Complete probe creation interface with multi-step workflow and WordNet integration"
      features:
        - "Multi-step workflow: config → sources → review → confirm → execute"
        - "Three word source types: custom lists, POS-pure words, WordNet synsets"
        - "Preset configurations for common demo scenarios (POS, semantic, disambiguation)"
        - "Real-time API integration with session creation and execution"
        - "Progress tracking with visual progress bars and time estimation"
        - "Synset ID validation and error handling"
        - "Layer selection controls with defaults"
        - "Category label preservation throughout workflow"
        - "Actual probe count display with execution time estimates"

    experiment_control_panel:
      status: todo
      priority: critical
      file: "frontend/src/components/ExperimentControlPanel.tsx"
      description: "Shared controls for word selection, color schemes, and analysis configuration"
      features:
        - name: "Word Selection Controls"
          status: todo
          notes: "Choose target tokens from probe data (e.g., 30-30 POS data + 'the' context)"
        - name: "Color Scheme Selector" 
          status: todo
          notes: "Dropdown for visualization coloring modes (semantic, POS, etc.)"
        - name: "Multiple Context Selection"
          status: todo
          notes: "Select multiple context tokens for comparative routing analysis"

    word_filter_panel:
      status: todo
      priority: high
      file: "frontend/src/components/WordFilterPanel.tsx"
      description: "Category-based filtering interface for context-target pairs"
      features:
        - name: "Context Categories Filter"
          status: todo
          notes: "Checkbox list with counts (e.g., 'determiner (1)', 'financial (12)')"
        - name: "Target Categories Filter" 
          status: todo
          notes: "Checkbox list with counts (e.g., 'animals (23)', 'nouns (30)')"
        - name: "Filter Summary Display"
          status: todo
          notes: "Show 'X/Y pairs selected' based on active filters"
        - name: "Clear All / Select All buttons"
          status: todo
          notes: "Quick filtering shortcuts for each category section"
        - name: "Category color indicators"
          status: todo
          notes: "Small colored dots next to category names matching visualization colors"

    # TAB 1: Expert Highways - Route exploration and clickable analysis
    expert_highways_tab:
      status: done
      priority: critical
      file: "frontend/src/pages/ExperimentPage.tsx"
      description: "Expert route analysis with API integration, flexible window support, and real-time statistics - integrated into ExperimentPage"
      features:
        - name: "Expert Routes API Integration"
          status: done
          notes: "Real-time API calls to backend expert route analysis service with loading/error states"
        - name: "Flexible Window Layer Support" 
          status: done
          notes: "Support for both 2-layer ([0,1], [1,2]) and 3-layer ([0,1,2], [1,2,3]) analysis windows"
        - name: "Real-time Route Statistics"
          status: done  
          notes: "Live display of route counts, probe coverage, and window analysis with dynamic updates"
        - name: "Expert Routes Visualization"
          status: todo
          notes: "ECharts Sankey component to replace current placeholder (data integration complete)"
        - name: "Clickable Route Interaction"
          status: todo
          notes: "Click route → show cohort for that route in details panel"
        - name: "Clickable Expert Interaction" 
          status: todo
          notes: "Click expert → show description of tokens/contexts it handles"
        - name: "Multiple Context Support"
          status: todo
          notes: "Select multiple contexts to show comparative route sets"
        - name: "Word Category Filtering Integration"
          status: todo
          notes: "Integrate WordFilterPanel in left sidebar for filtering context-target pairs"
        - name: "Blended Color Visualization"
          status: todo
          notes: "Apply blended coloring to Sankey visualization based on filtered target token categories"

    # TAB 2: Latent Space - Expert output analysis with clustering and PCA
    latent_space_tab:
      status: todo
      priority: critical
      file: "frontend/src/components/LatentSpaceTab.tsx"
      description: "Expert outputs analysis with clustering, PCA visualization, and trajectory analysis"
      features:
        - name: "Expert Output Clustering Interface"
          status: todo
          notes: "Controls for k-means/hierarchical clustering with k-per-layer configuration"
        - name: "3D PCA Visualization"
          status: todo
          notes: "Interactive 3D scatter plot of expert outputs with cluster coloring"
        - name: "Trajectory Analysis Integration"
          status: todo
          notes: "Real-time CTA integration for trajectory computation and path analysis"
        - name: "Cluster Detail Cards"
          status: todo
          notes: "Display cluster populations, centroids, and semantic labels from LLM analysis"
        - name: "Context-Target Trajectory Visualization"
          status: todo
          notes: "Show how contexts transform target trajectories in latent space"
        - name: "Word Category Filtering Integration"
          status: todo
          notes: "Integrate WordFilterPanel in left sidebar for filtering context-target pairs"
        - name: "Blended Color Visualization"
          status: todo
          notes: "Apply blended coloring to PCA plots and trajectory Sankey based on filtered categories"

    # SHARED PANELS - Used across both tabs
    llm_analysis_panel:
      status: todo
      priority: critical
      file: "frontend/src/components/LLMAnalysisPanel.tsx"
      description: "Shared AI-powered insights panel for expert specialization and trajectory analysis"
      features:
        - name: "Expert Specialization Analysis"
          status: todo
          notes: "LLM-generated descriptions of what each expert has learned to handle"
        - name: "Trajectory Narrative Generation"
          status: todo
          notes: "AI-generated coherent stories about concept evolution through latent space"
        - name: "Custom Question Interface"
          status: todo
          notes: "Allow researchers to ask specific questions about findings"
        - name: "Report Export Functionality"
          status: todo
          notes: "Generate and export comprehensive analysis reports"

    details_panel:
      status: todo
      priority: critical
      file: "frontend/src/components/DetailsPanel.tsx"
      description: "Context-sensitive details panel showing cohorts, expert descriptions, and cluster information"
      features:
        - name: "Cohort Display"
          status: todo
          notes: "Show word cohorts for selected routes/highways with category information"
        - name: "Expert Description Cards"
          status: todo
          notes: "Detailed information about clicked experts and their specializations"
        - name: "Cluster Information"
          status: todo
          notes: "Display cluster statistics, populations, and semantic labels"
        - name: "Category Analysis"
          status: todo
          notes: "Show category distributions and purity metrics for selections"
        - name: "Pie chart category breakdowns"
          status: todo
          notes: "Add pie charts to expert cards, highway cards, cluster cards, and route cards"
        - name: "Blended color indicators"
          status: todo
          notes: "Show computed blended colors alongside detailed category breakdowns"

    highway_panel:
      status: todo
      priority: critical
      file: "frontend/src/components/HighwayPanel.tsx"
      description: "Context Transformation View for analyzing how contexts transform target trajectories within highways."
      features:
        - name: "Highway selection from Expert Sankey clicks"
          status: todo
          notes: "Click highway edge → open Context Transformation View for that signature"
        - name: "Context→Target pairs display"
          status: todo
          notes: "Show context-target pairs that flow through this highway"
          format: |
            Highway: L6E2→L7E3→L8E1 (Context Transformation View)
            Context "the" → Target List: ["cat", "dog", "house"] (45% coverage)
            Context "a" → Target List: ["cat", "dog", "tree"] (32% coverage)
            [First Demo: Focus on single context → single target list]
        - name: "Context-target cohort export"
          status: todo
          notes: "Export cohort containing only context-target pairs for transformation clustering"
        - name: "Transformation clustering trigger"
          status: todo
          notes: "Cluster on first context to show how context shifts target trajectories in latent space"
        - name: "Context comparison visualization"
          status: todo
          notes: "Show how different contexts transform same targets differently within highway"

    latent_explorer:
      status: todo
      priority: critical
      file: "frontend/src/components/LatentExplorer.tsx"
      features:
        - name: "Window badge (read-only, Next/Prev disabled)"
          status: todo
        - name: "k-per-layer controls (first window only)"
          status: todo
        - name: "Latent Sankey + Path Drawer + Cluster Cards"
          status: todo

    visualizations:
      coloring_system:
        status: todo
        priority: high
        file: "frontend/src/utils/coloringUtils.ts"
        description: "Flexible coloring approaches for Sankey diagrams and 3D PCA plots"
        modes:
          - name: "Target Token Categories"
            description: "Color by target token categories (animals=green, nouns=blue, etc.)"
            notes: "Primary analysis focus - shows how target tokens route through expert highways"
          - name: "Context Token Categories"
            description: "Color by context token categories (determiner=purple, financial=red, etc.)"
            notes: "Shows how different contexts influence routing patterns"
          - name: "POS Comparison"
            description: "Color by part-of-speech ('the noun' vs 'the verb' routing)"
            notes: "Nouns=blue, Verbs=orange - shows grammatical routing differences"
          - name: "Embedding Distance"
            description: "Continuous gradient based on cosine similarity or PCA position"
            notes: "Smooth gradients reveal latent space organization"
          - name: "Activation Magnitude"
            description: "Heatmap coloring (high=red, low=blue) for activation strength"
            notes: "Shows 'hot' vs 'cold' expert paths"
          - name: "Context-Target Pairs (Future)"
            description: "Color by context-category → target-category combinations"
            notes: "Advanced mode: DETERMINER→ANIMAL vs FINANCIAL→AMBIGUOUS_WORDS get different colors"
        features:
          - name: "Interactive mode switching"
            status: todo
            notes: "Dropdown to switch coloring mode without reloading data"
          - name: "Blended color visualization"
            status: todo
            notes: "Mix colors based on category percentages using transparency/alpha blending (40% green + 35% blue + 25% red)"
          - name: "Category purity indicators"
            status: todo
            notes: "Opacity or saturation based on category purity percentage"
      
      sankey_charts:
        status: todo
        priority: high
        file: "frontend/src/components/charts/SankeyChart.tsx"
        library: "echarts"
        features:
          - name: "Expert highway flow visualization"
            status: todo
          - name: "Latent cluster flow visualization"
            status: todo
          - name: "Interactive edge selection"
            status: todo

      pca_3d:
        status: todo
        priority: medium
        file: "frontend/src/components/charts/PCA3D.tsx"
        library: "plotly"
        features:
          - name: "3D scatter plot of PCA128 features"
            status: todo
          - name: "Cluster color coding"
            status: todo
          - name: "Interactive exploration"
            status: todo

    cards:
      cluster_cards:
        status: todo
        priority: high
        file: "frontend/src/components/cards/ClusterCard.tsx"
        description: "Population + ETS (faithful) & CLR (lineage); LLM labels (dominant/secondary/outliers + provenance)"
        features:
          - name: "Population statistics"
            status: todo
          - name: "ETS rules (faithful thresholds + prec/cov + CIs)"
            status: todo
          - name: "CLR rules (lineage rules + prec/cov)"
            status: todo
          - name: "LLM-generated labels with provenance"
            status: todo
            format: "{ short_label, dominant, secondary, outliers, breakdown }"
          - name: "Dominant/secondary/outlier examples"
            status: todo
          - name: "Dominant/secondary/outlier token examples"
            status: todo
          - name: "Category breakdown pie chart"
            status: todo
            notes: "Small pie chart showing precise category percentages for detailed analysis"
          - name: "Blended color indicator"
            status: todo
            notes: "Show the computed blended color alongside pie chart for reference"

      expert_cards:
        status: todo
        priority: high
        file: "frontend/src/components/cards/ExpertCard.tsx"
        description: "Expert specialization information with category breakdown"
        features:
          - name: "Expert specialization display"
            status: todo
            notes: "Show what types of tokens/contexts this expert handles"
          - name: "Population and coverage statistics"
            status: todo
          - name: "Category breakdown pie chart"
            status: todo
            notes: "Show category composition for tokens routed to this expert"
          - name: "Blended color indicator"
            status: todo

      highway_cards:
        status: todo
        priority: high
        file: "frontend/src/components/cards/HighwayCard.tsx"
        description: "Highway route information with flow analysis"
        features:
          - name: "Route signature display"
            status: todo
            notes: "Show highway path (e.g., L6E2→L7E15→L8E7)"
          - name: "Flow coverage and statistics"
            status: todo
          - name: "Category breakdown pie chart"
            status: todo
            notes: "Show category composition for context-target pairs using this highway"
          - name: "Blended color indicator"
            status: todo

      route_cards:
        status: todo
        priority: high
        file: "frontend/src/components/cards/RouteCard.tsx"
        description: "Trajectory route information for latent space analysis"
        features:
          - name: "Trajectory path information"
            status: todo
            notes: "Show cluster transition paths and survival rates"
          - name: "Route coverage metrics"
            status: todo
          - name: "Category breakdown pie chart"
            status: todo
            notes: "Show category composition for tokens following this trajectory"
          - name: "Blended color indicator"
            status: todo

  pages:
    expert_routing_transform:
      status: todo
      priority: high
      file: "frontend/src/pages/ExpertRoutingTransform.tsx"
      description: "Expert Highway Routing Transformation: How contexts change MoE routing patterns."
      features:
        - name: "Routing similarity heatmap (ECharts)"
          status: todo
        - name: "Context comparison controls"
          status: todo
        - name: "Expert transition difference visualization"
          status: todo

# Core Algorithms
algorithms:
  window_selection:
    status: todo
    priority: critical
    description: "Auto-select first CTA window deterministically"
    implementation: "backend/src/algorithms/window_selection.py"
    algorithm: |
      def select_first_window(captured_layers):
          for L in sorted(captured_layers):
              if {L, L+1, L+2}.issubset(captured_layers):
                  return (L, L+1, L+2)
          raise ValueError("Need three consecutive layers")

  expert_highways:
    status: todo
    priority: high
    description: "Compute expert transition probabilities"
    implementation: "backend/src/algorithms/expert_analysis.py"
    metrics:
      - "P(Expert_l+1 | Expert_l)"
      - "Coverage percentage"
      - "Stickiness score"
      - "Ambiguity percentage"

  concept_trajectory:
    status: todo
    priority: critical
    description: "CTA analysis on selected cohort"
    implementation: "backend/src/algorithms/cta.py"
    features:
      - "Macro path computation"
      - "Survival analysis"
      - "Confusion matrices"
      - "Path coverage filtering (≥5%)"

# CLIs (thin wrappers; stdout prints output paths)
cli:
  probe:
    status: todo
    priority: medium
    file: "backend/src/cli/probe.py"
    command: "cmri probe --contexts ctx.csv --targets targets.csv --layers 6,7,8"

  exp_new:
    status: todo
    priority: medium
    file: "backend/src/cli/exp_new.py"
    command: "cmri exp-new --capture <capture_id>"

  exp_highways:
    status: todo
    priority: medium
    file: "backend/src/cli/exp_highways.py"
    command: "cmri exp-highways --exp <id>"

  exp_cohort:
    status: todo
    priority: medium
    file: "backend/src/cli/exp_cohort.py"
    command: 'cmri exp-cohort --exp <id> --highway "L6E2→L7E3→L8E1" --context "the"'

  exp_cluster:
    status: todo
    priority: medium
    file: "backend/src/cli/exp_cluster.py"
    command: "cmri exp-cluster --exp <id> --algo kmeans --k-per-layer L6=3 L7=4 L8=3"

  exp_cta:
    status: todo
    priority: medium
    file: "backend/src/cli/exp_cta.py"
    command: "cmri exp-cta --exp <id>"

  exp_rules:
    status: todo
    priority: medium
    file: "backend/src/cli/exp_rules.py"
    command: "cmri exp-rules --exp <id> --mode ets|clr"

  exp_label:
    status: todo
    priority: medium
    file: "backend/src/cli/exp_label.py"
    command: "cmri exp-label --exp <id>"

  transform_matrix:
    status: todo
    priority: medium
    file: "backend/src/cli/transform_matrix.py"
    command: "cmri transform-matrix --capture <capture_id> --contexts the,a,an"

# Data Schemas
schemas:
  tokens:
    status: done
    priority: critical
    file: "backend/src/schemas/tokens.py"
    fields: ["probe_id","context_text","target_text","context_token_id","target_token_id"]
    notes: "Simple index table linking probe_id to context-target pairs. MVP version without freq_bin/pos_guess."

  routing:
    status: done
    priority: critical
    file: "backend/src/schemas/routing.py"
    fields: ["probe_id","layer","expert_top4_ids","expert_top4_weights","expert_top1_id","expert_top1_weight","gate_entropy","routing_aux_loss","captured_at"]
    removed_fields: ["routing_weights","expert_output"]
    notes: "K=4 routing capture implemented with top-1 extraction. Includes validation and helper methods."

  features_pca128:
    status: done
    priority: critical
    file: "backend/src/schemas/features_pca128.py"
    fields: ["probe_id","layer","pca128"]
    notes: "Simplified PCA features schema for MVP. Version tracking deferred."

  expert_internal_activations:
    status: done
    priority: high
    file: "backend/src/schemas/expert_internal_activations.py"
    fields: ["probe_id","layer","expert_id","ff_intermediate_state","activation_dims","captured_at"]
    notes: "Expert internal FF activations implemented with validation and analysis methods"

  expert_output_states:
    status: done
    priority: high
    file: "backend/src/schemas/expert_output_states.py"
    fields: ["probe_id","layer","expert_output_state","post_expert_dims"]
    notes: "Collective expert output states implemented with cosine similarity and normalization methods"

  expert_internal_clusters:
    status: todo
    priority: high
    file: "backend/src/schemas/expert_internal_clusters.py"
    fields: ["experiment_id","layer","expert_id","cluster_id","cluster_center","population","specialization_label","pca_3d_coords"]
    notes: "Individual expert internal clustering results with 3D PCA coordinates"

  expert_output_clusters:
    status: todo
    priority: high
    file: "backend/src/schemas/expert_output_clusters.py"
    fields: ["experiment_id","cluster_id","cluster_center","population","concept_trajectory_label","pca_3d_coords","cta_integration_data"]
    notes: "Collective expert output clustering results with CTA integration data"

  llm_insights:
    status: todo
    priority: high
    file: "backend/src/schemas/llm_insights.py"
    fields: ["experiment_id","insight_type","generated_at","llm_model","insights_text","supporting_data","report_sections","visualization_configs"]
    notes: "LLM-generated insights and analysis reports with supporting evidence"

  capture_manifest:
    status: done
    priority: critical
    file: "backend/src/schemas/capture_manifest.py"
    fields: ["capture_session_id","session_name","contexts","targets","layers_captured","probe_count","created_at","model_name","context_category_assignments","target_category_assignments"]
    completed_features:
      - "Multi-category probe support with Dict[str, List[str]] format for preserving all category memberships"
      - "Parquet roundtrip serialization with to_parquet_dict() and from_parquet_dict() methods"
      - "WordNet integration for semantic category tracking with automatic category aggregation"
      - "UI-ready session metadata for experiment selection"
      - "Word source mining integration: custom lists, POS-pure words, WordNet hyponyms"
      - "Category preservation across multiple sources - words can belong to multiple categories"
    test_validation: "End-to-end tested with real GPT-OSS-20B captures, multi-category assignments survive full Parquet serialization cycle"

  cohort_manifest:
    status: todo
    priority: critical
    file: "backend/src/schemas/cohort_manifest.py"
    fields: ["schema_version","experiment_id","capture_id","highway_signature","context_tokens","target_tokens","cohort_mode","created_at"]
    cohort_modes:
      - name: "single_context"
        description: "Original workflow - single context analysis within highway"
      - name: "context_target_transformation"
        description: "Context-target transformation analysis - cluster on context to analyze target trajectory shifts"
      - name: "multi_context_highway"
        description: "Multi-context analysis for comparative transformation study"
    notes: "context_tokens and target_tokens arrays support context→target pairing for transformation analysis"

  experiment_manifest:
    status: todo
    priority: critical
    file: "backend/src/schemas/experiment_manifest.py"
    fields: ["schema_version","experiment_id","capture_id","window_used","clustering","created_at"]

  clustering:
    status: todo
    priority: critical
    file: "backend/src/schemas/clustering.py"
    fields: ["experiment_id", "layer", "cluster_id", "centroid", "population"]
    cluster_labeling_scheme:
      format: "L{layer}E{expert}C{cluster}{granularity}"
      examples: 
        - "L1E3C2M" (Layer 1, Expert 3, Cluster 2, Macro)
        - "L1E3C2m" (Layer 1, Expert 3, Cluster 2, micro)
      description: "Hierarchical identifier for cluster cards with macro/micro granularity"

  paths:
    status: todo
    priority: high
    file: "backend/src/schemas/paths.py"
    fields: ["path_signature", "coverage", "survival_rate", "examples"]

# Testing & Validation
testing:
  unit_tests:
    status: todo
    priority: medium
    coverage_target: "80%"
    files: ["tests/test_capture.py", "tests/test_clustering.py", "tests/test_cta.py"]

  integration_tests:
    status: done
    priority: medium
    completed_tests:
      - name: "Original capture system validation"
        file: "scripts/test_integrated_capture.py"
        status: done
        notes: "9/9 integration tests passing with real GPT-OSS-20B model, validates complete MoE capture pipeline"
      - name: "WordNet mining integration"
        file: "scripts/test_integrated_wordnet_mining.py"
        status: done
        notes: "Tests all 3 word mining sources (custom, POS-pure, synset hyponyms) with category assignment"
      - name: "Multi-category probe scenarios"
        file: "scripts/test_multi_category_probe.py"
        status: done
        notes: "Demo scenario validation: POS contrast (30 pairs), Semantic categories (537 pairs), Mixed sources (28 pairs)"
      - name: "End-to-end Parquet integration"
        file: "scripts/test_end_to_end_category_parquet.py"
        status: done
        notes: "Real GPT-OSS-20B captures with category assignments, Parquet serialization/deserialization validation"
    remaining_scenarios:
      - "End-to-end capture → highways → cohort → CTA"
      - "Window auto-selection edge cases"
      - "k-per-layer clustering validation"

  acceptance_tests:
    status: todo
    priority: high
    criteria:
      - "Capture: row counts match probes × layers"
      - "Expert Highways: edges sum correctly"
      - "Clustering: k-per-layer refreshes visualizations"
      - "Latent Explorer: ≥5% coverage paths rendered"
      - "Bundle: replay restores exact state"

# Deployment & DevOps
deployment:
  docker:
    status: todo
    priority: medium
    files: ["Dockerfile", "docker-compose.yml"]
    services: ["api", "frontend", "gpu-worker"]

  monitoring:
    status: todo  
    priority: low
    features:
      - "GPU memory monitoring"
      - "API response times"
      - "Error rate tracking"

# Demo Scenarios (Critical for Hackathon)
demo_scenarios:
  probe_creation_demonstration:
    status: todo
    priority: critical
    description: "Brief demonstration of probe creation with WordNet ontology integration"
    flow:
      - "Workspace page → New Probe button"
      - "Demonstrate WordNet integration as ontology source"
      - "Show custom word list creation option"
      - "Create probe session and execute MoE captures"
      - "Quick overview of resulting data structures"
    duration: "2-3 minutes"
    purpose: "Show foundation data collection capabilities"

  pos_analysis_demonstration:
    status: todo
    priority: critical
    description: "Main demo: 30-30 adjective/noun analysis with 'the' context using experiment view tabs"
    flow:
      - "Workspace page → New Experiment → Load POS probe data (30-30 adj/nouns + 'the' context)"
      - "Expert Highways Tab:"
      - "  - Show expert routing patterns for adjectives vs nouns with 'the' context"
      - "  - Click specific highway routes → Details panel shows cohorts"
      - "  - Click individual experts → Details panel shows expert specialization descriptions"  
      - "  - Demonstrate multiple context selection for comparative analysis"
      - "Latent Space Tab:"
      - "  - Show expert output clustering with k-per-layer controls"
      - "  - 3D PCA visualization with POS-based coloring (adj=blue, noun=green)"
      - "  - Trajectory analysis showing context-target transformation patterns"
      - "  - Cluster detail cards with LLM-generated semantic labels"
      - "LLM Analysis Panel (shared across tabs):"
      - "  - Expert specialization insights: what each expert learned to handle"
      - "  - Trajectory narratives: how contexts transform targets through latent space"
      - "  - Custom question interface for deeper analysis"
    duration: "8-10 minutes"
    purpose: "Demonstrate complete analysis workflow across both tab interfaces"

  two_experts_different_contexts:
    status: todo
    priority: post-mvp
    description: "Comparison demo: exploring two different experts with different context tokens"
    flow:
      - "Use existing capture with multiple contexts"
      - "Expert Explorer → Context filter (select 2 contexts)"
      - "Enable Diff toggle → show ΔP edges + Top Δ table"
      - "Compare expert routing patterns between contexts"
      - "Export cohorts for different highways"
      - "Side-by-side cluster analysis"

  expert_routing_transformation_analysis:
    status: todo
    priority: post-mvp
    description: "Expert Highway Routing Transformation: How contexts change MoE expert routing patterns"
    flow:
      - "Expert Routing Transform page → load capture contexts"
      - "Routing similarity heatmap → compare context routing patterns" 
      - "Expert transition differences → visualize how contexts change expert highways"
      - "Click patterns → navigate to specific context experiment"
      - "Demonstrate how context transforms MoE routing decisions"

  individual_highway_latent_analysis:
    status: todo
    priority: post-mvp
    description: "Individual Highway Latent Transformation: Context→target analysis within highway's space"
    flow:
      - "Expert Highways → click specific highway edge (e.g., L6E2→L7E3→L8E1)"
      - "Highway panel opens → shows context→target pairs using this highway"
      - "Select context 'the' → shows target list ['cat', 'dog', 'house']"
      - "Export highway-specific cohort → filtered by highway+context+targets"
      - "Run latent transformation clustering → cluster on context to show target trajectory shifts" 
      - "Visualize how context transforms target trajectories within this highway's latent space"
      - "Demonstrates context-driven transformation constrained to specific computational pathway"

  context_disambiguation_analysis:
    status: todo
    priority: high
    description: "Context Disambiguation Demo: How different contexts resolve ambiguous word meanings through expert routing"
    probe_configuration:
      contexts_by_category:
        financial: ["money", "financial", "investment"]
        medical: ["medical", "patient", "treatment"] 
        technology: ["computer", "software", "network"]
        legal: ["court", "legal", "judge"]
      ambiguous_targets: ["bank", "cell", "scale", "interest", "court", "patient"]
      total_probes: "12 contexts × 6 targets = 72 probes (~4-5 minutes)"
    expected_effects:
      - name: "bank disambiguation"
        examples: "'money' → 'bank' (financial) vs 'medical' → 'bank' (blood bank)"
        hypothesis: "Completely different expert highways based on context"
      - name: "cell specialization" 
        examples: "'medical' → 'cell' (biological) vs 'legal' → 'cell' (prison) vs 'computer' → 'cell' (spreadsheet)"
        hypothesis: "3-way expert routing comparison across domains"
      - name: "scale context sensitivity"
        examples: "'financial' → 'scale' (economic) vs 'medical' → 'scale' (symptom scale)"
        hypothesis: "Context-driven expert selection for same ambiguous word"
    flow:
      - "Create multi-category capture → 4 context categories × 6 ambiguous targets"
      - "Expert Routing Transform → disambiguation heatmap showing context effects"
      - "Select ambiguous word → compare expert highways across different contexts"
      - "Routing difference visualization → highlight which experts change based on context"
      - "Highway comparison → side-by-side analysis of 'money'→'bank' vs 'river'→'bank'"
      - "Demonstrate context-driven disambiguation at MoE expert granularity"
    demo_narrative: "Watch how expert highways change when we give the same ambiguous word different contexts. 'Bank' after 'money' routes through completely different experts than 'bank' after 'medical' - showing how context disambiguation happens at the expert level."

# Documentation
documentation:
  api_docs:
    status: todo
    priority: medium
    description: "FastAPI auto-generated OpenAPI docs"

  user_guide:
    status: todo
    priority: low
    file: "docs/user_guide.md"
    sections: ["Getting Started", "Expert Explorer", "Latent Explorer"]

  demo_recordings:
    status: todo
    priority: critical
    description: "Record all three demo scenarios for hackathon presentation"
    videos:
      - "Single highway exploration (core workflow)"
      - "Two-expert comparison with diff toggle"
      - "Transformation matrix → focused analysis"

# Dependencies & Environment
dependencies:
  python:
    version: "3.11"
    core: ["fastapi", "uvicorn", "transformers", "torch"]
    ml: ["scikit-learn", "numpy", "pandas"] 
    storage: ["pyarrow", "duckdb"]
    quantization: ["bitsandbytes", "accelerate"]
    nlp: ["nltk"]
    status: done
    notes: "All dependencies installed and working, FastAPI server operational with GPT-OSS-20B model integration"

  frontend:
    node_version: "18+"
    core: ["react", "typescript", "vite", "tailwindcss"]
    charts: ["echarts", "plotly.js"]
    status: done
    installed_versions:
      - "react: 19.1.1"
      - "typescript: 5.8.3" 
      - "vite: 7.1.2"
      - "tailwindcss: 4.1.13"
      - "echarts: 5.6.0, echarts-for-react: 3.0.2"
      - "plotly.js: 3.1.0, react-plotly.js: 2.6.0"
      - "react-router-dom: 7.8.2"
    notes: "All dependencies installed and working, development server operational with API client integration"

# Risk Mitigation
risks:
  gpu_memory:
    severity: high
    mitigation: "NF4 quantization + micro-batch floor=4"
    status: todo

  timeline_pressure:
    severity: medium
    mitigation: "First window only constraint"
    status: planned

  model_compatibility:
    severity: medium
    mitigation: "Flexible MoE detection and hooks"
    status: todo

# LLM Labeling Prompt Template (v1.4)
llm_labeling:
  cluster_labeling_prompt:
    template: |
      You are labeling a cluster of words from a language model's latent space.
      Task: provide a concise human-readable label and a structured breakdown.
      
      Input:
      - Cluster tokens: {token_list}
      - Population stats: {stats_summary}
      
      Instructions:
      1. Group by higher-order ontology (examples): 
         - Animate objects (animals, people, living beings)
         - Inanimate objects (vehicles, tools, artifacts, places, food items, …)
         - Abstract concepts (qualities, emotions, ideas, relations)
         - Function words (pronouns, determiners, auxiliaries, prepositions, …)
         - Other
      2. Identify dominant meaning, secondary meanings, and outliers.
      3. Return:
         - short_label (2–5 words)
         - breakdown (counts or %)
         - outliers list
      4. If uncertain, short_label = "Unknown" and return raw groups only.
      
      Output (JSON):
      {
        "short_label": "Animate Objects (Animals)",
        "dominant": "Animals",
        "secondary": ["Vehicles"],
        "outliers": ["sandwich"],
        "breakdown": {"Animals": 6, "Vehicles": 2, "Food": 1}
      }

  archetypal_path_labeling_prompt:
    template: |
      You are creating narrative labels for archetypal paths in a neural network's concept trajectory analysis.
      Task: Generate a coherent narrative that describes the conceptual journey through expert clusters.
      
      Input:
      - Path signature: {path_signature}
      - Cluster labels along path: {cluster_labels}
      - Coverage percentage: {coverage}
      - Example tokens: {example_tokens}
      
      Instructions:
      1. Analyze the progression of cluster labels from start to end of the path
      2. Identify the conceptual transformation or journey represented
      3. Create a narrative that explains WHY this path exists (what linguistic/semantic pattern it captures)
      4. Use the cluster labels as anchor points in your narrative
      5. Keep narratives concise but informative (2-4 sentences)
      
      Output (JSON):
      {
        "path_narrative": "Concrete nouns evolve through spatial relationships to abstract locational concepts",
        "transformation_type": "Semantic abstraction",
        "key_transition": "Physical objects → Spatial relations → Abstract location",
        "linguistic_pattern": "Noun phrase processing with increasing abstraction"
      }